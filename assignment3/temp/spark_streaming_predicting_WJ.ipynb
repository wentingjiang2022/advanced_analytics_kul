{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.103:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.103:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7fbb2ab6d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16319, 5)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"/Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_id</th>\n",
       "      <th>app_id</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>2786</td>\n",
       "      <td>2786</td>\n",
       "      <td>2786</td>\n",
       "      <td>2786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>13532</td>\n",
       "      <td>13532</td>\n",
       "      <td>13532</td>\n",
       "      <td>13514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  review_id  app_id  review_text\n",
       "label                                            \n",
       "0.0          2786       2786    2786         2786\n",
       "1.0         13532      13532   13532        13514"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"review_text\", StringType(), True),\n",
    "    StructField(\"label\", DoubleType(), True),\n",
    "    # add more columns here as needed\n",
    "])\n",
    "\n",
    "file_location=\"/Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\"\n",
    "#text_df = spark.read.text(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \n",
    "                                                     \"true\").option(\"multiLine\", \"true\").csv(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (19941, 5)\n"
     ]
    }
   ],
   "source": [
    "# Get the number of rows and columns in the DataFrame\n",
    "num_rows = text_df.count()\n",
    "num_cols = len(text_df.columns)\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(\"Shape of the DataFrame: (%d, %d)\" % (num_rows, num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 13:59:18 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, app_id, review_text, label\n",
      " Schema: _c0, review_id, app_id, review_text, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\n",
      "+---+-----------+-------+--------------------+-----+\n",
      "|_c0|  review_id| app_id|         review_text|label|\n",
      "+---+-----------+-------+--------------------+-----+\n",
      "|  0|136524320.0|2008820|i can confirm tha...|  1.0|\n",
      "|  1|136524365.0|2008820|Really good game,...|  1.0|\n",
      "|  2|116734222.0|1729900|Its not finished ...|  1.0|\n",
      "|  3|116989907.0|1729900|Hey. It's really ...|  1.0|\n",
      "|  4|136568878.0| 307950|          Fun so far|  1.0|\n",
      "+---+-----------+-------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "text_df = text_df.dropDuplicates()\n",
    "# Remove rows with missing values\n",
    "text_df = text_df.na.drop()\n",
    "# Remove rows with unexpected values\n",
    "text_df = text_df.filter(text_df[\"label\"]>=0)\n",
    "\n",
    "text_df = text_df.filter(text_df[\"label\"]<=1)\n",
    "\n",
    "# Drop multiple columns\n",
    "df = text_df.drop(\"_c0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 14:09:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, app_id, review_text, label\n",
      " Schema: _c0, review_id, app_id, review_text, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\n",
      "+-----------+-------+--------------------+-----+\n",
      "|  review_id| app_id|         review_text|label|\n",
      "+-----------+-------+--------------------+-----+\n",
      "|136578721.0|1811990|Amazing art style...|  1.0|\n",
      "|119483735.0|1527950|            dog shit|  0.0|\n",
      "|136601345.0|1091920|Very cute click g...|  1.0|\n",
      "|136613998.0|1527950|Similar to Battle...|  1.0|\n",
      "|136672689.0|1811990|I love the game s...|  0.0|\n",
      "+-----------+-------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 13:59:26 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, app_id, review_text, label\n",
      " Schema: _c0, review_id, app_id, review_text, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\n",
      "Shape of the DataFrame: (15281, 2)\n"
     ]
    }
   ],
   "source": [
    "# Get the number of rows and columns in the DataFrame\n",
    "num_rows = df.count()\n",
    "num_cols = len(df.columns)\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(\"Shape of the DataFrame: (%d, %d)\" % (num_rows, num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 13:59:31 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, app_id, review_text, label\n",
      " Schema: _c0, review_id, app_id, review_text, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"tokens\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=100, minCount=5, inputCol=\"tokens\", outputCol=\"embeddings\")\n",
    "model = word2Vec.fit(df)\n",
    "transformed_df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 13:59:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, app_id, review_text, label\n",
      " Schema: _c0, review_id, app_id, review_text, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|         review_text|label|              tokens|          embeddings|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|Amazing art style...|  1.0|[amazing, art, st...|[0.02256262019121...|\n",
      "|            dog shit|  0.0|         [dog, shit]|[-0.0909201018512...|\n",
      "|Very cute click g...|  1.0|[very, cute, clic...|[0.06723331050681...|\n",
      "|Similar to Battle...|  1.0|[similar, to, bat...|[0.03641914182797...|\n",
      "|I love the game s...|  0.0|[i, love, the, ga...|[-0.0270881544825...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 14:01:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, app_id, review_text, label\n",
      " Schema: _c0, review_id, app_id, review_text, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# had to convert string to numeric\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\", stringOrderType=\"frequencyDesc\")\n",
    "transformed_df = label_indexer.fit(transformed_df).transform(transformed_df)\n",
    "transformed_df = transformed_df.withColumn(\"label\", when(transformed_df[\"label\"] == 1, 0).otherwise(1))\n",
    "transformed_df = transformed_df.drop(\"label\")\n",
    "transformed_df = transformed_df.withColumnRenamed(\"label_index\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 14:05:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, app_id, review_text, label\n",
      " Schema: _c0, review_id, app_id, review_text, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 744:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unique_values = transformed_df.select(\"label\").distinct().collect()\n",
    "\n",
    "# Print the unique values\n",
    "for row in unique_values:\n",
    "    print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 14:05:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, app_id, review_text, label\n",
      " Schema: _c0, review_id, app_id, review_text, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\n",
      "23/05/02 14:05:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, app_id, review_text, label\n",
      " Schema: _c0, review_id, app_id, review_text, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 14:06:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, app_id, review_text, label\n",
      " Schema: _c0, review_id, app_id, review_text, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\n",
      "23/05/02 14:06:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, app_id, review_text, label\n",
      " Schema: _c0, review_id, app_id, review_text, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import DenseMatrix\n",
    "\n",
    "train_data, test_data = transformed_df.randomSplit([0.8, 0.2], seed=123)\n",
    "# Create a random forest classifier and fit it to the training data\n",
    "rf = RandomForestClassifier(featuresCol=\"embeddings\", labelCol=\"label\", numTrees=10)\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = rf_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 14:06:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, app_id, review_text, label\n",
      " Schema: _c0, review_id, app_id, review_text, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 778:>                                                        (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution:\n",
      "Class 0: 10209\n",
      "Class 1: 1978\n",
      "23/05/02 14:06:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, app_id, review_text, label\n",
      " Schema: _c0, review_id, app_id, review_text, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\n",
      "Test class distribution:\n",
      "Class 0: 2579\n",
      "Class 1: 515\n"
     ]
    }
   ],
   "source": [
    "# Compute the class distribution of the train data\n",
    "train_class_distribution = train_data.groupBy(\"label\").count().orderBy(\"label\").collect()\n",
    "print(\"Train class distribution:\")\n",
    "for row in train_class_distribution:\n",
    "    print(\"Class %d: %d\" % (row[\"label\"], row[\"count\"]))\n",
    "\n",
    "# Compute the class distribution of the test data\n",
    "test_class_distribution = test_data.groupBy(\"label\").count().orderBy(\"label\").collect()\n",
    "print(\"Test class distribution:\")\n",
    "for row in test_class_distribution:\n",
    "    print(\"Class %d: %d\" % (row[\"label\"], row[\"count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 14:06:25 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, app_id, review_text, label\n",
      " Schema: _c0, review_id, app_id, review_text, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 804:>                                                        (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 14:06:26 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, app_id, review_text, label\n",
      " Schema: _c0, review_id, app_id, review_text, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/wentingjiang/Desktop/Projects/advanced_analytics_kul/assignment3/reviews.csv\n",
      "Accuracy: 0.8335488041370395\n",
      "F1 score: 0.7578785002183763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 808:>                                                        (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy and F1 score of the predictions\n",
    "accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\").evaluate(predictions)\n",
    "f1_score = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\").evaluate(predictions)\n",
    "\n",
    "# Print the accuracy and F1 score\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "# Toy predict function that returns a random probability. Normally you'd use your loaded globals()['my_model'] here\n",
    "def predict(df):\n",
    "    return random.random()\n",
    "\n",
    "predict_udf = udf(predict, StringType())\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    \n",
    "    # Utilize our predict function\n",
    "    df_withpreds = df.withColumn(\"pred\", predict_udf(\n",
    "        struct([df[x] for x in df.columns])\n",
    "    ))\n",
    "    df_withpreds.show()\n",
    "    \n",
    "    # Normally, you wouldn't use a UDF (User Defined Function) Python function to predict as we did here (you can)\n",
    "    # but an MLlib model you've built and saved with Spark\n",
    "    # In this case, you need to prevent loading your model in every call to \"process\" as follows:\n",
    "    \n",
    "    # Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = '***' # Replace '***' with:    [...].load('my_logistic_regression')\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "    # And then predict using the loaded model (uncomment below):\n",
    "    \n",
    "    # df_result = globals()['my_model'].transform(df)\n",
    "    # df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 12:33:41 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/02 12:33:41 WARN BlockManager: Block input-0-1683023620800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/02 12:33:46 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/02 12:33:46 WARN BlockManager: Block input-0-1683023626000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/02 12:33:49 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/02 12:33:49 WARN BlockManager: Block input-0-1683023629000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/02 12:33:50 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/02 12:33:50 WARN BlockManager: Block input-0-1683023630000 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                  (0 + 1) / 1][Stage 1:>                  (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2023-05-02 12:33:50 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|1493750|    1|137664363|       its nice game|\n",
      "|1321440|    1|137664455|Nostalgic feel wh...|\n",
      "|1321440|    1|137664392|Very very good Po...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+--------------------+------------------+\n",
      "| app_id|label|review_id|         review_text|              pred|\n",
      "+-------+-----+---------+--------------------+------------------+\n",
      "|1493750|    1|137664363|       its nice game|0.6565145595554269|\n",
      "|1321440|    1|137664455|Nostalgic feel wh...|0.5485765583870403|\n",
      "|1321440|    1|137664392|Very very good Po...|0.7773911340974549|\n",
      "+-------+-----+---------+--------------------+------------------+\n",
      "\n",
      "23/05/02 12:33:55 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/02 12:33:55 WARN BlockManager: Block input-0-1683023635000 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 12:33:56 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/02 12:33:56 WARN BlockManager: Block input-0-1683023635800 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/02 12:33:59 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/02 12:33:59 WARN BlockManager: Block input-0-1683023639000 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2023-05-02 12:34:00 =========\n",
      "23/05/02 12:34:00 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/02 12:34:00 WARN BlockManager: Block input-0-1683023640000 replicated to only 0 peer(s) instead of 1 peers\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|1321440|    1|137664016|At first, I'd ass...|\n",
      "|1321440|    1|137663856|Might be one of i...|\n",
      "|1321440|    1|137663302|This game is Awes...|\n",
      "|2358520|    1|137663676|[b][i]A spiritual...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-------+-----+---------+--------------------+-------------------+\n",
      "| app_id|label|review_id|         review_text|               pred|\n",
      "+-------+-----+---------+--------------------+-------------------+\n",
      "|1321440|    1|137664016|At first, I'd ass...|  0.745487900932301|\n",
      "|1321440|    1|137663856|Might be one of i...|0.04498138095119997|\n",
      "|1321440|    1|137663302|This game is Awes...|  0.577938675852273|\n",
      "|2358520|    1|137663676|[b][i]A spiritual...| 0.7054709868572854|\n",
      "+-------+-----+---------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 12:34:01 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/02 12:34:01 WARN BlockManager: Block input-0-1683023641000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/02 12:34:02 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/02 12:34:02 WARN BlockManager: Block input-0-1683023642000 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2023-05-02 12:34:10 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|1869590|    1|137664702|           tomboy gf|\n",
      "|1869590|    1|137664488|the game itself i...|\n",
      "|1869590|    1|137663723|This game is so a...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-------+-----+---------+--------------------+-------------------+\n",
      "| app_id|label|review_id|         review_text|               pred|\n",
      "+-------+-----+---------+--------------------+-------------------+\n",
      "|1869590|    1|137664702|           tomboy gf|0.41284213761536503|\n",
      "|1869590|    1|137664488|the game itself i...| 0.9063430352795419|\n",
      "|1869590|    1|137663723|This game is so a...|0.21920045015905576|\n",
      "+-------+-----+---------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 12:34:28 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/02 12:34:28 WARN BlockManager: Block input-0-1683023668200 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2023-05-02 12:34:30 =========\n",
      "+-------+-----+---------+------------------+\n",
      "| app_id|label|review_id|       review_text|\n",
      "+-------+-----+---------+------------------+\n",
      "|1607240|    1|137663740|soks for president|\n",
      "+-------+-----+---------+------------------+\n",
      "\n",
      "+-------+-----+---------+------------------+------------------+\n",
      "| app_id|label|review_id|       review_text|              pred|\n",
      "+-------+-----+---------+------------------+------------------+\n",
      "|1607240|    1|137663740|soks for president|0.9786878975368976|\n",
      "+-------+-----+---------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 12:34:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/02 12:34:31 WARN BlockManager: Block input-0-1683023671000 replicated to only 0 peer(s) instead of 1 peers\n",
      "23/05/02 12:34:35 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "23/05/02 12:34:35 WARN BlockManager: Block input-0-1683023675000 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    }
   ],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n",
      "23/05/02 12:43:28 WARN StreamingContext: StreamingContext has already been stopped\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
