{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.171:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.171:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x154d4ad6320>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32041, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.read_csv(\"reviews.csv\", sep='\\t')\n",
    "df = df[['review_id', 'app_id', 'review_text', 'label']]\n",
    "df.to_csv('reviews.csv', sep='\\t', index=False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>app_id</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>6381</td>\n",
       "      <td>6381</td>\n",
       "      <td>6380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>25659</td>\n",
       "      <td>25659</td>\n",
       "      <td>25615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_id  app_id  review_text\n",
       "label                                \n",
       "0.0         6381    6381         6380\n",
       "1.0        25659   25659        25615"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8008426966292135"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer, HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "file_location=\"reviews.csv\"\n",
    "#text_df = spark.read.text(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"assignment 3\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|         review_text|label|\n",
      "+--------------------+-----+\n",
      "|i can confirm tha...|  1.0|\n",
      "|Really good game,...|  1.0|\n",
      "|Its not finished ...|  1.0|\n",
      "|Hey. It's really ...|  1.0|\n",
      "|          Fun so far|  1.0|\n",
      "|I'd rather play W...|  0.0|\n",
      "|I have been playi...|  1.0|\n",
      "|Nice game! Loads ...|  1.0|\n",
      "|All hail NA serve...|  1.0|\n",
      "|enjoying it so fa...|  1.0|\n",
      "|This game came as...|  1.0|\n",
      "|         Great fun  |  1.0|\n",
      "|Fantastic consept...|  1.0|\n",
      "|You know the game...|  1.0|\n",
      "|EDIT: My issues h...|  1.0|\n",
      "|AMAZIIIIIIIIIIING...|  1.0|\n",
      "|In its current st...|  0.0|\n",
      "|Играю в Калибр го...|  1.0|\n",
      "|Secret Word: Prou...|  1.0|\n",
      "|Fated Word: Death...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#text_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\", \"true\").csv(file_location)\n",
    "\n",
    "# read csv\n",
    "\n",
    "text_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .csv(file_location)\n",
    "\n",
    "text_df = text_df.select(col('review_text'), col('label'))\n",
    "text_df = text_df.dropna()\n",
    "text_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (31282, 2)\n"
     ]
    }
   ],
   "source": [
    "# Get the number of rows and columns in the DataFrame\n",
    "num_rows = text_df.count()\n",
    "num_cols = len(text_df.columns)\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(\"Shape of the DataFrame: (%d, %d)\" % (num_rows, num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "text_df = text_df.dropDuplicates()\n",
    "# Remove rows with missing values\n",
    "text_df = text_df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               label|\n",
      "+--------------------+\n",
      "|                 1.0|\n",
      "|                 0.0|\n",
      "|It's been at leas...|\n",
      "|[td]✔️ Exciting p...|\n",
      "|- I literally onl...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.select(\"label\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after filtering:  28366\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with unexpected labels\n",
    "text_df = text_df.filter((col(\"label\") == 1.0) | (col(\"label\") == 0.0))\n",
    "print(\"Number of rows after filtering: \", text_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|         review_text|label|\n",
      "+--------------------+-----+\n",
      "|\\n\\n\\n\\n\\n boring...|  0.0|\n",
      "|\\n\\nEven though I...|  1.0|\n",
      "|\\nUbisoft began m...|  1.0|\n",
      "|   As a big fan o...|  1.0|\n",
      "|  Cyberconnect2 f...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = text_df.limit(10000).randomSplit([0.8, 0.2], seed=7)\n",
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\", locale=\"en_US\")\n",
    "count_vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\")\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "string_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
    "\n",
    "# create model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define params grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "   .addGrid(count_vectorizer.vocabSize, [1000, 5000]) \\\n",
    "   .addGrid(lr.regParam, [0.01, 0.1]) \\\n",
    "   .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "   .build()\n",
    "\n",
    "# define the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_index\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, idf, string_indexer, lr])\n",
    "\n",
    "# define the cross-validator\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# fit pipeline to the training data\n",
    "cv_model = cv.fit(train_data)\n",
    "\n",
    "# make predictions on the test data\n",
    "predictions = cv_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8722\n",
      "F1 score: 0.8649\n",
      "Recall: 0.8722\n",
      "Precision: 0.8634\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_index\", predictionCol=\"prediction\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_score = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "# Calculate recall\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Calculate precision\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"F1 score: {:.4f}\".format(f1_score))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.3.2-bin-hadoop2\\python\\pyspark\\sql\\context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[1572.   83.]\n",
      " [ 177.  202.]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Convert the predictions and labels to an RDD\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"label_index\").rdd.map(lambda r: (r[0], r[1]))\n",
    "\n",
    "# Instantiate a MulticlassMetrics object\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "# Get the confusion matrix as a NumPy array\n",
    "confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from the cross-validation process\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Save the my_model\n",
    "best_model.save(\"my_logistic_regression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "global results\n",
    "results = []\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df = df.withColumn(\"label\", col(\"label\").cast(\"float\"))\n",
    "    df.show()\n",
    "    \n",
    "#     # Utilize our predict function\n",
    "#     df_withpreds = df.withColumn(\"pred\", predict_udf(\n",
    "#         struct([df[x] for x in df.columns])\n",
    "#     ))\n",
    "#     df_withpreds.show()\n",
    "    \n",
    "    # Normally, you wouldn't use a UDF (User Defined Function) Python function to predict as we did here (you can)\n",
    "    # but an MLlib model you've built and saved with Spark\n",
    "    # In this case, you need to prevent loading your model in every call to \"process\" as follows:\n",
    "    \n",
    "    # Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = PipelineModel.load('my_logistic_regression')\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "    # And then predict using the loaded model (uncomment below):\n",
    "    \n",
    "    df_result = globals()['my_model'].transform(df)\n",
    "    df_result.select('label', 'review_text', 'prediction','probability', 'label_index').show()\n",
    "    \n",
    "    collected_results = df_result.select('prediction', 'label_index').collect()\n",
    "    results.extend(collected_results)\n",
    "    \n",
    "    # If we have collected 10 results, show the data and clear the results list\n",
    "    display_results(results)\n",
    "\n",
    "\n",
    "def display_results(results):\n",
    "    result_df = spark.createDataFrame(results)\n",
    "    predictionAndLabels = result_df.select(\"prediction\", \"label_index\").rdd.map(lambda r: (r[0], r[1]))\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2023-05-22 00:57:30 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|2201030|  0.0|138713769|Crashed on exitin...|\n",
      "|1940340|  1.0|138714529|Definitely a lot ...|\n",
      "|1940340|  1.0|138713157|Better than darke...|\n",
      "|1159690|  1.0|138716330|Resource gatherin...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|label|         review_text|prediction|         probability|label_index|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|  0.0|Crashed on exitin...|       0.0|[0.59787048871053...|        1.0|\n",
      "|  1.0|Definitely a lot ...|       0.0|[0.97840684253083...|        0.0|\n",
      "|  1.0|Better than darke...|       1.0|[0.07603015005317...|        0.0|\n",
      "|  1.0|Resource gatherin...|       0.0|[0.76132775419668...|        0.0|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "\n",
      "Confusion matrix:\n",
      "[[2. 1.]\n",
      " [1. 0.]]\n",
      "========= 2023-05-22 00:57:40 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|1159690|  1.0|138716066|          Good Stuff|\n",
      "|1159690|  1.0|138715602|Will write a long...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|label|         review_text|prediction|         probability|label_index|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|  1.0|          Good Stuff|       0.0|[0.93614125420098...|        0.0|\n",
      "|  1.0|Will write a long...|       0.0|[0.94336778554974...|        0.0|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "\n",
      "Confusion matrix:\n",
      "[[4. 1.]\n",
      " [1. 0.]]\n",
      "========= 2023-05-22 00:57:50 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|1230170|  1.0|138713872|Fun concept. I ca...|\n",
      "|1230170|  1.0|138713013|Damn this game is...|\n",
      "| 669330|  1.0|138714975|It's a bit rough ...|\n",
      "|2369390|  0.0|138715915|Sync does not wor...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|label|         review_text|prediction|         probability|label_index|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|  1.0|Fun concept. I ca...|       1.0|[0.00447372184942...|        0.0|\n",
      "|  1.0|Damn this game is...|       0.0|[0.94101916395527...|        0.0|\n",
      "|  1.0|It's a bit rough ...|       0.0|[0.95189990004632...|        0.0|\n",
      "|  0.0|Sync does not wor...|       0.0|[0.83275938855119...|        1.0|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "\n",
      "Confusion matrix:\n",
      "[[6. 2.]\n",
      " [2. 0.]]\n",
      "========= 2023-05-22 00:58:20 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "| 855740|  1.0|138716034|This game is way ...|\n",
      "|1559600|  1.0|138714523|This scratches my...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|label|         review_text|prediction|         probability|label_index|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|  1.0|This game is way ...|       0.0|[0.84981449606162...|        0.0|\n",
      "|  1.0|This scratches my...|       0.0|[0.92989649787380...|        0.0|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "\n",
      "Confusion matrix:\n",
      "[[8. 2.]\n",
      " [2. 0.]]\n",
      "========= 2023-05-22 00:58:30 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|2234020|  1.0|138716386|I've played all s...|\n",
      "|2159650|  0.0|138714637|Fun for the first...|\n",
      "| 605740|  0.0|138716142|Вы там совсем гол...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|label|         review_text|prediction|         probability|label_index|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|  1.0|I've played all s...|       0.0|[0.84582664029250...|        0.0|\n",
      "|  0.0|Fun for the first...|       1.0|[0.30043056860282...|        1.0|\n",
      "|  0.0|Вы там совсем гол...|       0.0|[0.86618000513816...|        1.0|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "\n",
      "Confusion matrix:\n",
      "[[9. 2.]\n",
      " [3. 1.]]\n",
      "========= 2023-05-22 00:58:40 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "| 605740|  1.0|138714673|This is the best ...|\n",
      "|1581480|  1.0|138716038|               11/10|\n",
      "| 824600|  1.0|138716621|Czechoslovakian D...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|label|         review_text|prediction|         probability|label_index|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|  1.0|This is the best ...|       0.0|[0.90229973150246...|        0.0|\n",
      "|  1.0|               11/10|       0.0|[0.86618000513816...|        0.0|\n",
      "|  1.0|Czechoslovakian D...|       0.0|[0.88561878407387...|        0.0|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "\n",
      "Confusion matrix:\n",
      "[[12.  2.]\n",
      " [ 3.  1.]]\n",
      "========= 2023-05-22 00:58:50 =========\n",
      "+------+-----+---------+--------------------+\n",
      "|app_id|label|review_id|         review_text|\n",
      "+------+-----+---------+--------------------+\n",
      "|824600|  1.0|138715930|I bought this gam...|\n",
      "|824600|  1.0|138715536|Very fun, hope th...|\n",
      "+------+-----+---------+--------------------+\n",
      "\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|label|         review_text|prediction|         probability|label_index|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|  1.0|I bought this gam...|       0.0|[0.99951824908360...|        0.0|\n",
      "|  1.0|Very fun, hope th...|       0.0|[0.97179059668722...|        0.0|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "\n",
      "Confusion matrix:\n",
      "[[14.  2.]\n",
      " [ 3.  1.]]\n",
      "========= 2023-05-22 00:59:20 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|2391910|  1.0|138715573|This is great\\nTh...|\n",
      "| 673750|  1.0|138715554|The reason for th...|\n",
      "|1268750|  1.0|138716931|DID YOU SAY CITIZ...|\n",
      "|1268750|  1.0|138716887|Not a lot of cont...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|label|         review_text|prediction|         probability|label_index|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|  1.0|This is great\\nTh...|       0.0|[0.99517616825376...|        0.0|\n",
      "|  1.0|The reason for th...|       0.0|[0.65419119514737...|        0.0|\n",
      "|  1.0|DID YOU SAY CITIZ...|       0.0|[0.88453886000722...|        0.0|\n",
      "|  1.0|Not a lot of cont...|       0.0|[0.99652619536389...|        0.0|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "\n",
      "Confusion matrix:\n",
      "[[18.  2.]\n",
      " [ 3.  1.]]\n",
      "========= 2023-05-22 00:59:30 =========\n"
     ]
    }
   ],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|1268750|  1.0|138716814|A good bug is a d...|\n",
      "|1268750|  1.0|138716802|I mean, it's fun!...|\n",
      "|1268750|  1.0|138716796|Fun game kinda la...|\n",
      "|1268750|  1.0|138716788|DO YOU HAVE WHAT ...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|label|         review_text|prediction|         probability|label_index|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|  1.0|A good bug is a d...|       0.0|[0.82004547123984...|        0.0|\n",
      "|  1.0|I mean, it's fun!...|       0.0|[0.99541394247558...|        0.0|\n",
      "|  1.0|Fun game kinda la...|       0.0|[0.57054455388059...|        0.0|\n",
      "|  1.0|DO YOU HAVE WHAT ...|       0.0|[0.87886141189681...|        0.0|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "\n",
      "Confusion matrix:\n",
      "[[22.  2.]\n",
      " [ 3.  1.]]\n",
      "========= 2023-05-22 00:59:40 =========\n",
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|1268750|  1.0|138716694|Potential is ther...|\n",
      "|1268750|  1.0|138716668|Besides that some...|\n",
      "+-------+-----+---------+--------------------+\n",
      "\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|label|         review_text|prediction|         probability|label_index|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "|  1.0|Potential is ther...|       0.0|[0.83206276727315...|        0.0|\n",
      "|  1.0|Besides that some...|       0.0|[0.99979167974294...|        0.0|\n",
      "+-----+--------------------+----------+--------------------+-----------+\n",
      "\n",
      "Confusion matrix:\n",
      "[[24.  2.]\n",
      " [ 3.  1.]]\n",
      "========= 2023-05-22 01:00:10 =========\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
